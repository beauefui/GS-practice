# GS-Practice: ä» Gemma Scope å­¦ä¹  SAE

ä¸€ä¸ªåŠ¨æ‰‹å­¦ä¹ é¡¹ç›®ï¼ŒåŸºäº Google DeepMind çš„ [Gemma Scope](https://deepmind.google/discover/blog/gemma-scope-helping-the-safety-community-shed-light-on-the-inner-workings-of-language-models/) é¡¹ç›®ï¼Œç†è§£å¹¶å®ç° **ç¨€ç–è‡ªç¼–ç å™¨ (Sparse Autoencoders, SAEs)**ã€‚

## ğŸ¯ é¡¹ç›®ç›®æ ‡

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªä¸ªäººå­¦ä¹ ç»ƒä¹ ï¼Œæ—¨åœ¨ï¼š

1. **ç†è§£ JumpReLU SAE æ¶æ„** â€” å­¦ä¹ ç¨€ç–è‡ªç¼–ç å™¨å¦‚ä½•å°†ç¥ç»ç½‘ç»œçš„æ¿€æ´»å€¼åˆ†è§£ä¸ºå¯è§£é‡Šçš„ç‰¹å¾
2. **ä»é›¶å®ç° SAE** â€” å‚è€ƒ [Gemma Scope 2 Tutorial](https://colab.research.google.com/drive/1NhWjg7n0nhfW--CjtsOdw5A5J_-Bzn4r)ï¼Œé€æ­¥æ„å»º SAE æ ¸å¿ƒç»„ä»¶
3. **åœ¨ Gemma æ¨¡å‹æ¿€æ´»å€¼ä¸Šè®­ç»ƒ SAE** â€” å®è·µå®Œæ•´æµç¨‹ï¼šæ¿€æ´»å€¼æå– â†’ SAE è®­ç»ƒ â†’ è¯„ä¼°
4. **æ¢ç´¢å¯è§£é‡Šæ€§æŠ€æœ¯** â€” ç‰¹å¾å¯è§†åŒ–ã€æ¨¡å‹å¼•å¯¼ (Steering)ã€é‡å»ºè´¨é‡æŒ‡æ ‡

## ğŸ“š èƒŒæ™¯çŸ¥è¯†

### ä»€ä¹ˆæ˜¯ç¨€ç–è‡ªç¼–ç å™¨ (SAE)ï¼Ÿ

SAE æ˜¯ä¸€ç§æ— ç›‘ç£æ¨¡å‹ï¼Œç”¨äºå°†ç¥ç»ç½‘ç»œçš„å†…éƒ¨æ¿€æ´»å€¼åˆ†è§£ä¸ºä¸€ç»„**ç¨€ç–çš„ã€è¿‡å®Œå¤‡çš„å¯è§£é‡Šç‰¹å¾**ã€‚æ ¸å¿ƒæ€æƒ³ï¼š

- **ç¼–ç å™¨ (Encoder)**ï¼šå°†æ¨¡å‹æ¿€æ´»å€¼ï¼ˆç»´åº¦ `d_model`ï¼‰æ˜ å°„åˆ°æ›´é«˜ç»´çš„æ½œç©ºé—´ï¼ˆç»´åº¦ `d_sae`ï¼Œå…¶ä¸­ `d_sae >> d_model`ï¼‰
- **ç¨€ç–æ€§ (Sparsity)**ï¼šå¯¹äºä»»æ„ç»™å®šè¾“å…¥ï¼Œåªæœ‰å°‘é‡æ½œåœ¨ç‰¹å¾è¢«æ¿€æ´»
- **è§£ç å™¨ (Decoder)**ï¼šä»ç¨€ç–çš„æ½œåœ¨è¡¨ç¤ºé‡å»ºåŸå§‹æ¿€æ´»å€¼

### ä»€ä¹ˆæ˜¯ JumpReLUï¼Ÿ

Gemma Scope ä½¿ç”¨ **JumpReLU** æ¿€æ´»å‡½æ•°æ›¿ä»£æ ‡å‡† ReLUã€‚JumpReLU ä¸ºæ¯ä¸ªç‰¹å¾å¼•å…¥ä¸€ä¸ªå¯å­¦ä¹ çš„**é˜ˆå€¼** â€” ä½äºé˜ˆå€¼çš„é¢„æ¿€æ´»å€¼è¢«ç½®é›¶ã€‚ä¼˜åŠ¿åŒ…æ‹¬ï¼š

- æ›´å¥½åœ°æ§åˆ¶ç¨€ç–æ€§ï¼ˆç›´æ¥ä¼˜åŒ– L0ï¼‰
- åœ¨ç›¸åŒç¨€ç–åº¦ä¸‹è·å¾—æ›´é«˜çš„é‡å»ºä¿çœŸåº¦
- æ¯” TopK æˆ– Gated SAE æ–¹æ¡ˆæä¾›æ›´æ¸…æ™°çš„ç‰¹å¾åˆ†ç¦»

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
GS-practice/
â”œâ”€â”€ README.md                  # æœ¬æ–‡ä»¶
â”œâ”€â”€ requirements.txt           # Python ä¾èµ–
â”œâ”€â”€ .gitignore                 # Git å¿½ç•¥è§„åˆ™
â”œâ”€â”€ src/                       # æ ¸å¿ƒæºä»£ç 
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model.py               # JumpReLU SAE æ¨¡å‹å®šä¹‰ (encode/decode/forward)
â”‚   â”œâ”€â”€ hooks.py               # é€šè¿‡ forward hooks æå–æ¨¡å‹æ¿€æ´»å€¼
â”‚   â”œâ”€â”€ metrics.py             # è¯„ä¼°æŒ‡æ ‡ (L0, FVU, MSE, Dead Features)
â”‚   â”œâ”€â”€ train.py               # æŸå¤±å‡½æ•° + è®­ç»ƒå¾ªç¯ + æ¿€æ´»å€¼æ”¶é›†
â”‚   â””â”€â”€ utils.py               # å·¥å…·å‡½æ•° (æ¨¡å‹åŠ è½½/æƒé‡ä¸‹è½½/checkpoint)
â”œâ”€â”€ scripts/                   # å…¥å£è„šæœ¬
â”‚   â”œâ”€â”€ train_sae.py           # è®­ç»ƒå…¥å£ (CLI, æ”¯æŒ --smoke-test)
â”‚   â””â”€â”€ eval_sae.py            # è¯„ä¼°å…¥å£ (æŒ‡æ ‡æŠ¥å‘Š + Top ç‰¹å¾åˆ†æ)
â”œâ”€â”€ configs/                   # è®­ç»ƒé…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ default.yaml           # é»˜è®¤è¶…å‚é…ç½®
â”œâ”€â”€ sae/                       # ä¿å­˜çš„ SAE æƒé‡å’Œ checkpoint
â””â”€â”€ model/                     # ç¼“å­˜çš„åŸºåº§æ¨¡å‹æ–‡ä»¶
```

## ğŸ”¬ å­¦ä¹ è·¯çº¿

- [ ] **é˜¶æ®µ 1ï¼šSAE æ¨ç†** â€” åŠ è½½é¢„è®­ç»ƒçš„ Gemma Scope SAE æƒé‡ï¼Œè¿è¡Œæ¨ç†
- [ ] **é˜¶æ®µ 2ï¼šæ¿€æ´»å€¼æå–** â€” Hook è¿› Gemma æ¨¡å‹å„å±‚ï¼Œæå–æ¿€æ´»å€¼
- [ ] **é˜¶æ®µ 3ï¼šSAE è®­ç»ƒ** â€” å®ç° JumpReLU æŸå¤±å‡½æ•°çš„è®­ç»ƒå¾ªç¯ï¼ˆé‡å»º + ç¨€ç–æ€§ï¼‰
- [ ] **é˜¶æ®µ 4ï¼šè¯„ä¼°** â€” è®¡ç®— L0ã€FVU å’Œ Delta Loss æŒ‡æ ‡
- [ ] **é˜¶æ®µ 5ï¼šå¯è§£é‡Šæ€§å®éªŒ** â€” å¯è§†åŒ– top-activating ç‰¹å¾ï¼Œå®éªŒ Steering

## ğŸ”§ å‰ç½®æ¡ä»¶

- Python 3.10+
- CUDA 12.xï¼ˆA800 GPU è®­ç»ƒï¼‰
- ~16GB+ GPU æ˜¾å­˜ï¼ˆç”¨äº Gemma æ¨¡å‹ + SAEï¼‰
- HuggingFace Tokenï¼ˆä¸‹è½½ Gemma æ¨¡å‹éœ€è¦ï¼‰

## ğŸš€ å®Œæ•´ä½¿ç”¨æµç¨‹

### Step 0ï¼šç¯å¢ƒæ­å»º

```bash
git clone https://github.com/beauefui/GS-practice.git
cd GS-practice
conda create -n gs python=3.10 -y
conda activate gs
pip install torch --index-url https://download.pytorch.org/whl/cu121
pip install -r requirements.txt
```

### Step 1ï¼šä¸‹è½½æƒé‡

```bash
# å°† <YOUR_HF_TOKEN> æ›¿æ¢ä¸ºä½ çš„ token
# ä¸‹è½½ Gemma æ¨¡å‹ + Google é¢„è®­ç»ƒ SAE æƒé‡
python scripts/download_weights.py --token <YOUR_HF_TOKEN>
```

**å¾—åˆ°ï¼š** `model/gemma-3-1b-pt/` (Gemma åŸºåº§æ¨¡å‹) å’Œ `sae/gemma-scope-2-1b-pt/` (Google é¢„è®­ç»ƒ SAE æƒé‡)

### Step 2ï¼šSmoke Testï¼ˆéªŒè¯ç¯å¢ƒï¼‰

```bash
python scripts/train_sae.py --smoke-test
python scripts/eval_sae.py --smoke-test
```

**å¾—åˆ°ï¼š** ä½¿ç”¨éšæœºæ•°æ®è·‘å‡ æ­¥è®­ç»ƒå’Œè¯„ä¼°ï¼Œç¡®è®¤ç¯å¢ƒæ— é—®é¢˜

---

### ğŸ…°ï¸ ä¸»è·¯çº¿ï¼šä½¿ç”¨ Google é¢„è®­ç»ƒ SAE è¯„ä¼°ï¼ˆæ¨èï¼‰

> è¿™æ˜¯ä¸ [Colab æ•™ç¨‹](https://colab.research.google.com/drive/1NhWjg7n0nhfW--CjtsOdw5A5J_-Bzn4r) å¯¹é½çš„ç”¨æ³•ã€‚
> ç›´æ¥åŠ è½½ Google èŠ±å¤§é‡ç®—åŠ›è®­ç»ƒå¥½çš„ SAE æƒé‡ï¼Œå¯¹ Gemma æ¨¡å‹è¿›è¡Œåˆ†æã€‚

```bash
# ç›´æ¥è¯„ä¼° Google é¢„è®­ç»ƒ SAE
CUDA_VISIBLE_DEVICES=0 python scripts/eval_sae.py --pretrained
```

**è¿‡ç¨‹ï¼š**
1. åŠ è½½ Gemma æ¨¡å‹ + Google é¢„è®­ç»ƒ SAE æƒé‡ (`sae/gemma-scope-2-1b-pt/`)
2. æå–æ¿€æ´»å€¼ â†’ é€šè¿‡ SAE ç¼–ç /è§£ç  â†’ è®¡ç®—è¯„ä¼°æŒ‡æ ‡

**å¾—åˆ°ï¼š**
- ç»ˆç«¯æ‰“å°è¯„ä¼°æŠ¥å‘Šï¼ˆL0 ç¨€ç–åº¦ã€FVU é‡å»ºè´¨é‡ã€Top-10 æ´»è·ƒç‰¹å¾ï¼‰
- è‡ªåŠ¨ç”Ÿæˆ `reports/report_<æ—¶é—´æˆ³>.md` å’Œ `.json` æŠ¥å‘Šæ–‡ä»¶
- é¢„æœŸæ•ˆæœï¼š**L0 â‰ˆ 70, FVU â‰ˆ 2-3%**ï¼ˆä¸ Colab æ•™ç¨‹ä¸€è‡´ï¼‰

---

### ğŸ…±ï¸ å¯é€‰è·¯çº¿ï¼šä»é›¶è®­ç»ƒ SAEï¼ˆå­¦ä¹ ç”¨ï¼‰

> è¿™æ¡è·¯çº¿æ˜¯ä¸ºäº†**ç†è§£ SAE è®­ç»ƒè¿‡ç¨‹**ï¼Œæ•ˆæœè¿œä¸å¦‚ Google é¢„è®­ç»ƒç‰ˆæœ¬ï¼Œ
> ä½†å¯¹å­¦ä¹  SAE çš„å·¥ä½œåŸç†éå¸¸æœ‰å¸®åŠ©ã€‚

**è®­ç»ƒï¼š**

```bash
CUDA_VISIBLE_DEVICES=0 python scripts/train_sae.py --config configs/default.yaml
```

**è¯„ä¼°è‡ªè®­ç»ƒçš„ checkpointï¼š**

```bash
CUDA_VISIBLE_DEVICES=0 python scripts/eval_sae.py --checkpoint sae/checkpoints/checkpoint_final.pt
```

**è°ƒå‚ï¼ˆç¼–è¾‘ `configs/default.yaml`ï¼‰ï¼š**

```yaml
model:
  hook_layer: 22        # è¦ hook çš„å±‚ (0-25)
sae:
  d_sae: 16384          # SAE å®½åº¦
training:
  num_steps: 20000      # è®­ç»ƒæ­¥æ•°
  sparsity_coeff: 0.01  # ç¨€ç–æ€§å¼ºåº¦ (è¶Šå¤§è¶Šç¨€ç–)
  lr: 1e-4              # å­¦ä¹ ç‡
```

## ğŸ”„ åˆ‡æ¢ä¸åŒçš„ Gemma æ¨¡å‹å’Œ SAE

### å¯ç”¨çš„æ¨¡å‹å’Œå¯¹åº”çš„ Gemma Scope

æ¯ä¸ª Gemma åŸºåº§æ¨¡å‹éƒ½æœ‰å¯¹åº”çš„ Gemma Scope SAE æƒé‡ï¼ˆ`-pt` = é¢„è®­ç»ƒç‰ˆï¼Œ`-it` = æŒ‡ä»¤å¾®è°ƒç‰ˆï¼‰ï¼š

| Gemma åŸºåº§æ¨¡å‹ | å¯¹åº” Gemma Scope | å±‚æ•° | d_model | æ˜¾å­˜éœ€æ±‚ |
|---------------|-----------------|------|---------|---------|
| `google/gemma-3-270m-pt` | `google/gemma-scope-2-270m-pt` | 18 | 1536 | ~2 GB |
| `google/gemma-3-1b-pt` â† **å½“å‰** | `google/gemma-scope-2-1b-pt` | 26 | 1152 | ~4 GB |
| `google/gemma-3-4b-pt` | `google/gemma-scope-2-4b-pt` | 34 | 2560 | ~10 GB |
| `google/gemma-3-12b-pt` | `google/gemma-scope-2-12b-pt` | 48 | 3840 | ~28 GB |
| `google/gemma-3-27b-pt` | `google/gemma-scope-2-27b-pt` | 62 | 4608 | ~60 GB |

> æŠŠ `-pt` æ¢æˆ `-it` å³å¯ä½¿ç”¨æŒ‡ä»¤å¾®è°ƒç‰ˆæœ¬ï¼ˆå¦‚ `gemma-3-4b-it` + `gemma-scope-2-4b-it`ï¼‰

### åˆ‡æ¢æ­¥éª¤

**ä»¥åˆ‡æ¢åˆ° 4B æ¨¡å‹ä¸ºä¾‹ï¼š**

#### 1. ä¿®æ”¹ä¸‹è½½è„šæœ¬ `scripts/download_weights.py`

```python
# æ”¹ repo_id å’Œ local_dir
snapshot_download(
    repo_id="google/gemma-3-4b-pt",          # â† æ”¹è¿™é‡Œ
    local_dir="model/gemma-3-4b-pt",          # â† æ”¹è¿™é‡Œ
    token=args.token,
)

snapshot_download(
    repo_id="google/gemma-scope-2-4b-pt",     # â† æ”¹è¿™é‡Œ
    local_dir="sae/gemma-scope-2-4b-pt",      # â† æ”¹è¿™é‡Œ
    allow_patterns=["resid_post/layer_20_width_65k_l0_medium/*"],  # â† æ”¹å±‚å·
    token=args.token,
)
```

#### 2. ä¿®æ”¹é…ç½®æ–‡ä»¶ `configs/default.yaml`

```yaml
model:
  name: "model/gemma-3-4b-pt"     # â† æ”¹æ¨¡å‹è·¯å¾„
  hook_layer: 20                   # â† æ”¹å±‚å· (é€šå¸¸é€‰ä¸­é—´ååçš„å±‚)

pretrained_sae:
  repo_id: "google/gemma-scope-2-4b-pt"    # â† æ”¹ scope ä»“åº“
  local_dir: "sae/gemma-scope-2-4b-pt"     # â† æ”¹æœ¬åœ°è·¯å¾„
  layer: 20                                 # â† å’Œ hook_layer ä¸€è‡´
  width: "65k"
  l0: "medium"
```

#### 3. é‡æ–°ä¸‹è½½å¹¶è¯„ä¼°

```bash
python scripts/download_weights.py --token <YOUR_HF_TOKEN>
CUDA_VISIBLE_DEVICES=0 python scripts/eval_sae.py --pretrained
```

### SAE å˜ä½“é€‰æ‹©

æ¯ä¸ªå±‚ä¸‹æœ‰ä¸åŒå®½åº¦å’Œç¨€ç–åº¦çš„ SAE å¯é€‰ï¼Œåœ¨ `allow_patterns` å’Œ `configs/default.yaml` ä¸­ä¿®æ”¹ï¼š

| å‚æ•° | å¯é€‰å€¼ | è¯´æ˜ |
|------|-------|------|
| `width` | `16k`, `65k`, `262k`, `1m` | ç‰¹å¾æ•°é‡ï¼Œè¶Šå¤§è¶Šç»†ç²’åº¦ |
| `l0` | `small`, `medium`, `big` | ç›®æ ‡ç¨€ç–åº¦ï¼Œsmall=æ›´ç¨€ç– |

> ä¾‹å¦‚ `layer_15_width_262k_l0_small` è¡¨ç¤ºç¬¬ 15 å±‚ã€262k ç‰¹å¾ã€é«˜ç¨€ç–åº¦

## ğŸ¦™ æ‹“å±•ï¼šä» Gemma Scope åˆ° Llama Scope

[Llama Scope](https://github.com/OpenMOSS/Language-Model-SAEs) æ˜¯ OpenMOSS å›¢é˜Ÿä¸º **Llama-3.1-8B** è®­ç»ƒçš„ SAE å¥—ä»¶ï¼Œæä¾›äº†æ‰€æœ‰å±‚å’Œå­å±‚çš„ 256 ä¸ª TopK SAEã€‚

### Gemma Scope vs Llama Scope æ ¸å¿ƒåŒºåˆ«

| | Gemma Scope | Llama Scope |
|---|---|---|
| **åŸºåº§æ¨¡å‹** | Gemma 3 (270M ~ 27B) | Llama 3.1 8B |
| **SAE æ¶æ„** | JumpReLU (å¯å­¦ä¹ é˜ˆå€¼) | **TopK** (å›ºå®šé€‰ top-k ä¸ªç‰¹å¾) |
| **æƒé‡æ¥æº** | Google å®˜æ–¹ HuggingFace | `fnlp/Llama-Scope` (OpenMOSS) |
| **ç‰¹å¾æ•°é‡** | 16k / 65k / 262k / 1m | **32k (8x)** / 128k (32x) |
| **æ¡†æ¶** | è‡ªå®šä¹‰ä»£ç å³å¯ | æ¨èä½¿ç”¨ `lm-saes` æ¡†æ¶ |
| **å‘½åè§„åˆ™** | `layer_22_width_65k_l0_medium` | `L22R-8x` (å±‚å·+ä½ç½®+å€ç‡) |

### Llama Scope å‘½åè§„åˆ™

`L[å±‚å·][ä½ç½®]-[å€ç‡]x`ï¼Œä¾‹å¦‚ï¼š

| åç§° | å«ä¹‰ |
|------|------|
| `L15R-8x` | ç¬¬ 15 å±‚ï¼Œpost-MLP **R**esidual streamï¼Œ8x æ‰©å±• (32k ç‰¹å¾) |
| `L15A-8x` | ç¬¬ 15 å±‚ï¼Œ**A**ttention outputï¼Œ8x æ‰©å±• |
| `L15M-8x` | ç¬¬ 15 å±‚ï¼Œ**M**LP outputï¼Œ8x æ‰©å±• |
| `L15R-32x` | ç¬¬ 15 å±‚ï¼ŒResidualï¼Œ32x æ‰©å±• (128k ç‰¹å¾ï¼Œä¸æ¨èï¼Œæ­»ç‰¹å¾å¤š) |

### ä½¿ç”¨æ–¹å¼

Llama Scope æ¨èä½¿ç”¨å®˜æ–¹çš„ `lm-saes` æ¡†æ¶ï¼Œè€Œä¸æ˜¯æˆ‘ä»¬çš„è‡ªå®šä¹‰ä»£ç ï¼š

```bash
# å®‰è£… lm-saes æ¡†æ¶
pip install lm-saes==2.0.0b16
```

åŸºæœ¬ç”¨æ³•å‚è€ƒ [lm-saes examples](https://github.com/OpenMOSS/Language-Model-SAEs/tree/main/examples)ã€‚

### å¦‚æœè¦ç”¨æˆ‘ä»¬çš„ä»£ç åŠ è½½ Llama Scope æƒé‡

**éœ€è¦ä¿®æ”¹çš„ä»£ç ï¼š**

#### 1. `src/model.py` â€” æ·»åŠ  TopK æ¿€æ´»å‡½æ•°

```python
# Llama Scope ä½¿ç”¨ TopK è€Œé JumpReLU
# TopK: åªä¿ç•™å‰ k ä¸ªæœ€å¤§çš„æ¿€æ´»å€¼ï¼Œå…¶ä½™ç½®é›¶
def topk_activation(pre_acts, k=64):
    topk_vals, topk_idx = pre_acts.topk(k, dim=-1)
    acts = torch.zeros_like(pre_acts)
    acts.scatter_(-1, topk_idx, topk_vals)
    return acts
```

#### 2. `src/utils.py` â€” ä¿®æ”¹æƒé‡åŠ è½½è·¯å¾„

```python
# Llama Scope æƒé‡ä¸‹è½½
from huggingface_hub import snapshot_download
snapshot_download(
    repo_id="fnlp/Llama-Scope",
    allow_patterns=["L15R-8x/*"],  # æŒ‰éœ€é€‰æ‹©å±‚å’Œä½ç½®
    local_dir="sae/llama-scope",
)
```

#### 3. `src/hooks.py` â€” å±‚è®¿é—®è·¯å¾„ä¸å˜

Llama å’Œ Gemma æ¨¡å‹ç»“æ„ç±»ä¼¼ï¼Œéƒ½æ˜¯ `model.model.layers[i]`ï¼Œ**hooks ä»£ç ä¸éœ€è¦æ”¹**ã€‚

### å¯¹ç…§å‚è€ƒ

| æ¥æº | é“¾æ¥ |
|------|------|
| **Llama Scope è®ºæ–‡** | [Llama Scope: Extracting Millions of Features from Llama-3.1-8B](https://arxiv.org/abs/2410.20526) |
| **è®­ç»ƒæ¡†æ¶** | [github.com/OpenMOSS/Language-Model-SAEs](https://github.com/OpenMOSS/Language-Model-SAEs) |
| **é¢„è®­ç»ƒæƒé‡** | [huggingface.co/fnlp/Llama-Scope](https://huggingface.co/fnlp/Llama-Scope) |


## ğŸ“¦ ä¸»è¦ä¾èµ–

| åŒ…å | ç”¨é€” |
|------|------|
| `torch` | æ·±åº¦å­¦ä¹ æ¡†æ¶ |
| `transformers` | ä» HuggingFace åŠ è½½ Gemma æ¨¡å‹ |
| `huggingface_hub` | ä¸‹è½½ SAE æƒé‡ |
| `safetensors` | é«˜æ•ˆçš„æƒé‡åºåˆ—åŒ–æ ¼å¼ |
| `einops` | å¼ é‡è¿ç®— |
| `pyyaml` | é…ç½®æ–‡ä»¶è§£æ |
| `datasets` | åŠ è½½è®­ç»ƒæ•°æ®é›† |
| `wandb` | å®éªŒè¿½è¸ªï¼ˆå¯é€‰ï¼‰ |

## ğŸ“– å‚è€ƒèµ„æ–™

- **Gemma Scope åšå®¢**: [deepmind.google/gemma-scope](https://deepmind.google/discover/blog/gemma-scope-helping-the-safety-community-shed-light-on-the-inner-workings-of-language-models/)
- **Gemma Scope 2 Tutorial (Colab)**: [colab.research.google.com](https://colab.research.google.com/drive/1NhWjg7n0nhfW--CjtsOdw5A5J_-Bzn4r)
- **JumpReLU è®ºæ–‡**: [Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders](https://arxiv.org/abs/2407.14435)
- **Gemma Scope æƒé‡ (HuggingFace)**: [google/gemma-scope-2b-pt-res](https://huggingface.co/google/gemma-scope-2b-pt-res)
- **SAELens (è®­ç»ƒåº“)**: [github.com/jbloomAus/SAELens](https://github.com/jbloomAus/SAELens)

## âš–ï¸ è®¸å¯

æœ¬é¡¹ç›®ä»…ç”¨äºä¸ªäººå­¦ä¹ å’Œç ”ç©¶ç›®çš„ã€‚
