# GS-Practice: ä» Gemma Scope å­¦ä¹  SAE

ä¸€ä¸ªåŠ¨æ‰‹å­¦ä¹ é¡¹ç›®ï¼ŒåŸºäº Google DeepMind çš„ [Gemma Scope](https://deepmind.google/discover/blog/gemma-scope-helping-the-safety-community-shed-light-on-the-inner-workings-of-language-models/) é¡¹ç›®ï¼Œç†è§£å¹¶å®ç° **ç¨€ç–è‡ªç¼–ç å™¨ (Sparse Autoencoders, SAEs)**ã€‚

## ğŸ¯ é¡¹ç›®ç›®æ ‡

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªä¸ªäººå­¦ä¹ ç»ƒä¹ ï¼Œæ—¨åœ¨ï¼š

1. **ç†è§£ JumpReLU SAE æ¶æ„** â€” å­¦ä¹ ç¨€ç–è‡ªç¼–ç å™¨å¦‚ä½•å°†ç¥ç»ç½‘ç»œçš„æ¿€æ´»å€¼åˆ†è§£ä¸ºå¯è§£é‡Šçš„ç‰¹å¾
2. **ä»é›¶å®ç° SAE** â€” å‚è€ƒ [Gemma Scope 2 Tutorial](https://colab.research.google.com/drive/1NhWjg7n0nhfW--CjtsOdw5A5J_-Bzn4r)ï¼Œé€æ­¥æ„å»º SAE æ ¸å¿ƒç»„ä»¶
3. **åœ¨ Gemma æ¨¡å‹æ¿€æ´»å€¼ä¸Šè®­ç»ƒ SAE** â€” å®è·µå®Œæ•´æµç¨‹ï¼šæ¿€æ´»å€¼æå– â†’ SAE è®­ç»ƒ â†’ è¯„ä¼°
4. **æ¢ç´¢å¯è§£é‡Šæ€§æŠ€æœ¯** â€” ç‰¹å¾å¯è§†åŒ–ã€æ¨¡å‹å¼•å¯¼ (Steering)ã€é‡å»ºè´¨é‡æŒ‡æ ‡

## ğŸ“š èƒŒæ™¯çŸ¥è¯†

### ä»€ä¹ˆæ˜¯ç¨€ç–è‡ªç¼–ç å™¨ (SAE)ï¼Ÿ

SAE æ˜¯ä¸€ç§æ— ç›‘ç£æ¨¡å‹ï¼Œç”¨äºå°†ç¥ç»ç½‘ç»œçš„å†…éƒ¨æ¿€æ´»å€¼åˆ†è§£ä¸ºä¸€ç»„**ç¨€ç–çš„ã€è¿‡å®Œå¤‡çš„å¯è§£é‡Šç‰¹å¾**ã€‚æ ¸å¿ƒæ€æƒ³ï¼š

- **ç¼–ç å™¨ (Encoder)**ï¼šå°†æ¨¡å‹æ¿€æ´»å€¼ï¼ˆç»´åº¦ `d_model`ï¼‰æ˜ å°„åˆ°æ›´é«˜ç»´çš„æ½œç©ºé—´ï¼ˆç»´åº¦ `d_sae`ï¼Œå…¶ä¸­ `d_sae >> d_model`ï¼‰
- **ç¨€ç–æ€§ (Sparsity)**ï¼šå¯¹äºä»»æ„ç»™å®šè¾“å…¥ï¼Œåªæœ‰å°‘é‡æ½œåœ¨ç‰¹å¾è¢«æ¿€æ´»
- **è§£ç å™¨ (Decoder)**ï¼šä»ç¨€ç–çš„æ½œåœ¨è¡¨ç¤ºé‡å»ºåŸå§‹æ¿€æ´»å€¼

### ä»€ä¹ˆæ˜¯ JumpReLUï¼Ÿ

Gemma Scope ä½¿ç”¨ **JumpReLU** æ¿€æ´»å‡½æ•°æ›¿ä»£æ ‡å‡† ReLUã€‚JumpReLU ä¸ºæ¯ä¸ªç‰¹å¾å¼•å…¥ä¸€ä¸ªå¯å­¦ä¹ çš„**é˜ˆå€¼** â€” ä½äºé˜ˆå€¼çš„é¢„æ¿€æ´»å€¼è¢«ç½®é›¶ã€‚ä¼˜åŠ¿åŒ…æ‹¬ï¼š

- æ›´å¥½åœ°æ§åˆ¶ç¨€ç–æ€§ï¼ˆç›´æ¥ä¼˜åŒ– L0ï¼‰
- åœ¨ç›¸åŒç¨€ç–åº¦ä¸‹è·å¾—æ›´é«˜çš„é‡å»ºä¿çœŸåº¦
- æ¯” TopK æˆ– Gated SAE æ–¹æ¡ˆæä¾›æ›´æ¸…æ™°çš„ç‰¹å¾åˆ†ç¦»

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
GS-practice/
â”œâ”€â”€ README.md                  # æœ¬æ–‡ä»¶
â”œâ”€â”€ requirements.txt           # Python ä¾èµ–
â”œâ”€â”€ .gitignore                 # Git å¿½ç•¥è§„åˆ™
â”œâ”€â”€ src/                       # æ ¸å¿ƒæºä»£ç 
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model.py               # JumpReLU SAE æ¨¡å‹å®šä¹‰ (encode/decode/forward)
â”‚   â”œâ”€â”€ hooks.py               # é€šè¿‡ forward hooks æå–æ¨¡å‹æ¿€æ´»å€¼
â”‚   â”œâ”€â”€ metrics.py             # è¯„ä¼°æŒ‡æ ‡ (L0, FVU, MSE, Dead Features)
â”‚   â”œâ”€â”€ train.py               # æŸå¤±å‡½æ•° + è®­ç»ƒå¾ªç¯ + æ¿€æ´»å€¼æ”¶é›†
â”‚   â””â”€â”€ utils.py               # å·¥å…·å‡½æ•° (æ¨¡å‹åŠ è½½/æƒé‡ä¸‹è½½/checkpoint)
â”œâ”€â”€ scripts/                   # å…¥å£è„šæœ¬
â”‚   â”œâ”€â”€ train_sae.py           # è®­ç»ƒå…¥å£ (CLI, æ”¯æŒ --smoke-test)
â”‚   â””â”€â”€ eval_sae.py            # è¯„ä¼°å…¥å£ (æŒ‡æ ‡æŠ¥å‘Š + Top ç‰¹å¾åˆ†æ)
â”œâ”€â”€ configs/                   # è®­ç»ƒé…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ default.yaml           # é»˜è®¤è¶…å‚é…ç½®
â”œâ”€â”€ sae/                       # ä¿å­˜çš„ SAE æƒé‡å’Œ checkpoint
â””â”€â”€ model/                     # ç¼“å­˜çš„åŸºåº§æ¨¡å‹æ–‡ä»¶
```

## ğŸ”¬ å­¦ä¹ è·¯çº¿

- [ ] **é˜¶æ®µ 1ï¼šSAE æ¨ç†** â€” åŠ è½½é¢„è®­ç»ƒçš„ Gemma Scope SAE æƒé‡ï¼Œè¿è¡Œæ¨ç†
- [ ] **é˜¶æ®µ 2ï¼šæ¿€æ´»å€¼æå–** â€” Hook è¿› Gemma æ¨¡å‹å„å±‚ï¼Œæå–æ¿€æ´»å€¼
- [ ] **é˜¶æ®µ 3ï¼šSAE è®­ç»ƒ** â€” å®ç° JumpReLU æŸå¤±å‡½æ•°çš„è®­ç»ƒå¾ªç¯ï¼ˆé‡å»º + ç¨€ç–æ€§ï¼‰
- [ ] **é˜¶æ®µ 4ï¼šè¯„ä¼°** â€” è®¡ç®— L0ã€FVU å’Œ Delta Loss æŒ‡æ ‡
- [ ] **é˜¶æ®µ 5ï¼šå¯è§£é‡Šæ€§å®éªŒ** â€” å¯è§†åŒ– top-activating ç‰¹å¾ï¼Œå®éªŒ Steering

## ğŸ”§ å‰ç½®æ¡ä»¶

- Python 3.10+
- CUDA 12.xï¼ˆA800 GPU è®­ç»ƒï¼‰
- ~16GB+ GPU æ˜¾å­˜ï¼ˆç”¨äº Gemma æ¨¡å‹ + SAEï¼‰
- HuggingFace Tokenï¼ˆä¸‹è½½ Gemma æ¨¡å‹éœ€è¦ï¼‰

## ğŸš€ å®Œæ•´ä½¿ç”¨æµç¨‹

### Step 0ï¼šç¯å¢ƒæ­å»º

```bash
git clone https://github.com/beauefui/GS-practice.git
cd GS-practice
conda create -n gs python=3.10 -y
conda activate gs
pip install torch --index-url https://download.pytorch.org/whl/cu121
pip install -r requirements.txt
```

### Step 1ï¼šä¸‹è½½æ¨¡å‹æƒé‡

```bash
# ä¸‹è½½ Gemma 3 1B åŸºåº§æ¨¡å‹ (~2GB)
huggingface-cli download google/gemma-3-1b-pt \
    --local-dir model/gemma-3-1b-pt --token <YOUR_HF_TOKEN>

# ä¸‹è½½ Gemma Scope SAE æƒé‡ (åªä¸‹éœ€è¦çš„å±‚)
huggingface-cli download google/gemma-scope-2-1b-pt \
    --include "resid_post/layer_22/width_65k_l0_medium/*" \
    --local-dir sae/gemma-scope-2-1b-pt --token <YOUR_HF_TOKEN>
```

**å¾—åˆ°ï¼š** `model/gemma-3-1b-pt/` å’Œ `sae/gemma-scope-2-1b-pt/` ç›®å½•ä¸‹æœ‰æ¨¡å‹æƒé‡æ–‡ä»¶

### Step 2ï¼šSmoke Testï¼ˆéªŒè¯ä»£ç èƒ½è·‘ï¼‰

```bash
python scripts/train_sae.py --smoke-test
python scripts/eval_sae.py --smoke-test
```

**å¾—åˆ°ï¼š** ä½¿ç”¨éšæœºæ•°æ®è·‘å‡ æ­¥è®­ç»ƒå’Œè¯„ä¼°ï¼Œç¡®è®¤ç¯å¢ƒæ— é—®é¢˜ã€‚ä¼šçœ‹åˆ° loss ä¸‹é™ + ä¸€ä»½è¯„ä¼°æŠ¥å‘Š

### Step 3ï¼šæ­£å¼è®­ç»ƒ SAE

```bash
python scripts/train_sae.py --config configs/default.yaml
```

**è¿‡ç¨‹ï¼š**
1. åŠ è½½ Gemma 3 1B æ¨¡å‹ â†’ æå–ç¬¬ 22 å±‚çš„æ¿€æ´»å€¼
2. é‡Šæ”¾ Gemma æ˜¾å­˜ â†’ åœ¨æ¿€æ´»å€¼ä¸Šè®­ç»ƒ JumpReLU SAEï¼ˆ50000 æ­¥ï¼‰
3. ç»ˆç«¯å®æ—¶æ‰“å° `loss / L0 / FVU`ï¼Œæ¯ 5000 æ­¥è‡ªåŠ¨ä¿å­˜ checkpoint

**å¾—åˆ°ï¼š** `sae/checkpoints/checkpoint_step_5000.pt`, `..._10000.pt`, ..., `checkpoint_final.pt`

### Step 4ï¼šè¯„ä¼°è®­ç»ƒç»“æœ

```bash
python scripts/eval_sae.py --checkpoint sae/checkpoints/checkpoint_final.pt
```

**å¾—åˆ°ï¼š**
- ç»ˆç«¯æ‰“å°è¯„ä¼°æŠ¥å‘Šï¼ˆL0 ç¨€ç–åº¦ã€FVU é‡å»ºè´¨é‡ã€Top-10 æ´»è·ƒç‰¹å¾ï¼‰
- è‡ªåŠ¨ç”Ÿæˆ `sae/checkpoints/report_<æ—¶é—´æˆ³>.md` å’Œ `.json` æ–‡ä»¶

### è°ƒå‚

ç¼–è¾‘ `configs/default.yaml` ä¿®æ”¹è¶…å‚æ•°ï¼š

```yaml
model:
  hook_layer: 22        # è¦ hook çš„å±‚ (0-25)
sae:
  d_sae: 16384          # SAE å®½åº¦
training:
  num_steps: 50000      # è®­ç»ƒæ­¥æ•°
  sparsity_coeff: 1e-3  # ç¨€ç–æ€§å¼ºåº¦ (è¶Šå¤§è¶Šç¨€ç–)
  lr: 3e-4              # å­¦ä¹ ç‡
```

## ğŸ“¦ ä¸»è¦ä¾èµ–

| åŒ…å | ç”¨é€” |
|------|------|
| `torch` | æ·±åº¦å­¦ä¹ æ¡†æ¶ |
| `transformers` | ä» HuggingFace åŠ è½½ Gemma æ¨¡å‹ |
| `huggingface_hub` | ä¸‹è½½ SAE æƒé‡ |
| `safetensors` | é«˜æ•ˆçš„æƒé‡åºåˆ—åŒ–æ ¼å¼ |
| `einops` | å¼ é‡è¿ç®— |
| `pyyaml` | é…ç½®æ–‡ä»¶è§£æ |
| `datasets` | åŠ è½½è®­ç»ƒæ•°æ®é›† |
| `wandb` | å®éªŒè¿½è¸ªï¼ˆå¯é€‰ï¼‰ |

## ğŸ“– å‚è€ƒèµ„æ–™

- **Gemma Scope åšå®¢**: [deepmind.google/gemma-scope](https://deepmind.google/discover/blog/gemma-scope-helping-the-safety-community-shed-light-on-the-inner-workings-of-language-models/)
- **Gemma Scope 2 Tutorial (Colab)**: [colab.research.google.com](https://colab.research.google.com/drive/1NhWjg7n0nhfW--CjtsOdw5A5J_-Bzn4r)
- **JumpReLU è®ºæ–‡**: [Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders](https://arxiv.org/abs/2407.14435)
- **Gemma Scope æƒé‡ (HuggingFace)**: [google/gemma-scope-2b-pt-res](https://huggingface.co/google/gemma-scope-2b-pt-res)
- **SAELens (è®­ç»ƒåº“)**: [github.com/jbloomAus/SAELens](https://github.com/jbloomAus/SAELens)

## âš–ï¸ è®¸å¯

æœ¬é¡¹ç›®ä»…ç”¨äºä¸ªäººå­¦ä¹ å’Œç ”ç©¶ç›®çš„ã€‚
