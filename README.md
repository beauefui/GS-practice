# GS-Practice: ä» Gemma Scope å­¦ä¹  SAE

ä¸€ä¸ªåŠ¨æ‰‹å­¦ä¹ é¡¹ç›®ï¼ŒåŸºäº Google DeepMind çš„ [Gemma Scope](https://deepmind.google/discover/blog/gemma-scope-helping-the-safety-community-shed-light-on-the-inner-workings-of-language-models/) é¡¹ç›®ï¼Œç†è§£å¹¶å®ç° **ç¨€ç–è‡ªç¼–ç å™¨ (Sparse Autoencoders, SAEs)**ã€‚

## ğŸ¯ é¡¹ç›®ç›®æ ‡

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªä¸ªäººå­¦ä¹ ç»ƒä¹ ï¼Œæ—¨åœ¨ï¼š

1. **ç†è§£ JumpReLU SAE æ¶æ„** â€” å­¦ä¹ ç¨€ç–è‡ªç¼–ç å™¨å¦‚ä½•å°†ç¥ç»ç½‘ç»œçš„æ¿€æ´»å€¼åˆ†è§£ä¸ºå¯è§£é‡Šçš„ç‰¹å¾
2. **ä»é›¶å®ç° SAE** â€” å‚è€ƒ [Gemma Scope 2 Tutorial](https://colab.research.google.com/drive/1NhWjg7n0nhfW--CjtsOdw5A5J_-Bzn4r)ï¼Œé€æ­¥æ„å»º SAE æ ¸å¿ƒç»„ä»¶
3. **åœ¨ Gemma æ¨¡å‹æ¿€æ´»å€¼ä¸Šè®­ç»ƒ SAE** â€” å®è·µå®Œæ•´æµç¨‹ï¼šæ¿€æ´»å€¼æå– â†’ SAE è®­ç»ƒ â†’ è¯„ä¼°
4. **æ¢ç´¢å¯è§£é‡Šæ€§æŠ€æœ¯** â€” ç‰¹å¾å¯è§†åŒ–ã€æ¨¡å‹å¼•å¯¼ (Steering)ã€é‡å»ºè´¨é‡æŒ‡æ ‡

## ğŸ“š èƒŒæ™¯çŸ¥è¯†

### ä»€ä¹ˆæ˜¯ç¨€ç–è‡ªç¼–ç å™¨ (SAE)ï¼Ÿ

SAE æ˜¯ä¸€ç§æ— ç›‘ç£æ¨¡å‹ï¼Œç”¨äºå°†ç¥ç»ç½‘ç»œçš„å†…éƒ¨æ¿€æ´»å€¼åˆ†è§£ä¸ºä¸€ç»„**ç¨€ç–çš„ã€è¿‡å®Œå¤‡çš„å¯è§£é‡Šç‰¹å¾**ã€‚æ ¸å¿ƒæ€æƒ³ï¼š

- **ç¼–ç å™¨ (Encoder)**ï¼šå°†æ¨¡å‹æ¿€æ´»å€¼ï¼ˆç»´åº¦ `d_model`ï¼‰æ˜ å°„åˆ°æ›´é«˜ç»´çš„æ½œç©ºé—´ï¼ˆç»´åº¦ `d_sae`ï¼Œå…¶ä¸­ `d_sae >> d_model`ï¼‰
- **ç¨€ç–æ€§ (Sparsity)**ï¼šå¯¹äºä»»æ„ç»™å®šè¾“å…¥ï¼Œåªæœ‰å°‘é‡æ½œåœ¨ç‰¹å¾è¢«æ¿€æ´»
- **è§£ç å™¨ (Decoder)**ï¼šä»ç¨€ç–çš„æ½œåœ¨è¡¨ç¤ºé‡å»ºåŸå§‹æ¿€æ´»å€¼

### ä»€ä¹ˆæ˜¯ JumpReLUï¼Ÿ

Gemma Scope ä½¿ç”¨ **JumpReLU** æ¿€æ´»å‡½æ•°æ›¿ä»£æ ‡å‡† ReLUã€‚JumpReLU ä¸ºæ¯ä¸ªç‰¹å¾å¼•å…¥ä¸€ä¸ªå¯å­¦ä¹ çš„**é˜ˆå€¼** â€” ä½äºé˜ˆå€¼çš„é¢„æ¿€æ´»å€¼è¢«ç½®é›¶ã€‚ä¼˜åŠ¿åŒ…æ‹¬ï¼š

- æ›´å¥½åœ°æ§åˆ¶ç¨€ç–æ€§ï¼ˆç›´æ¥ä¼˜åŒ– L0ï¼‰
- åœ¨ç›¸åŒç¨€ç–åº¦ä¸‹è·å¾—æ›´é«˜çš„é‡å»ºä¿çœŸåº¦
- æ¯” TopK æˆ– Gated SAE æ–¹æ¡ˆæä¾›æ›´æ¸…æ™°çš„ç‰¹å¾åˆ†ç¦»

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
GS-practice/
â”œâ”€â”€ README.md                  # æœ¬æ–‡ä»¶
â”œâ”€â”€ requirements.txt           # Python ä¾èµ–
â”œâ”€â”€ .gitignore                 # Git å¿½ç•¥è§„åˆ™
â”œâ”€â”€ src/                       # æ ¸å¿ƒæºä»£ç 
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model.py               # JumpReLU SAE æ¨¡å‹å®šä¹‰ (encode/decode/forward)
â”‚   â”œâ”€â”€ hooks.py               # é€šè¿‡ forward hooks æå–æ¨¡å‹æ¿€æ´»å€¼
â”‚   â”œâ”€â”€ metrics.py             # è¯„ä¼°æŒ‡æ ‡ (L0, FVU, MSE, Dead Features)
â”‚   â”œâ”€â”€ train.py               # æŸå¤±å‡½æ•° + è®­ç»ƒå¾ªç¯ + æ¿€æ´»å€¼æ”¶é›†
â”‚   â””â”€â”€ utils.py               # å·¥å…·å‡½æ•° (æ¨¡å‹åŠ è½½/æƒé‡ä¸‹è½½/checkpoint)
â”œâ”€â”€ scripts/                   # å…¥å£è„šæœ¬
â”‚   â”œâ”€â”€ train_sae.py           # è®­ç»ƒå…¥å£ (CLI, æ”¯æŒ --smoke-test)
â”‚   â””â”€â”€ eval_sae.py            # è¯„ä¼°å…¥å£ (æŒ‡æ ‡æŠ¥å‘Š + Top ç‰¹å¾åˆ†æ)
â”œâ”€â”€ configs/                   # è®­ç»ƒé…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ default.yaml           # é»˜è®¤è¶…å‚é…ç½®
â”œâ”€â”€ sae/                       # ä¿å­˜çš„ SAE æƒé‡å’Œ checkpoint
â””â”€â”€ model/                     # ç¼“å­˜çš„åŸºåº§æ¨¡å‹æ–‡ä»¶
```

## ğŸ”¬ å­¦ä¹ è·¯çº¿

- [ ] **é˜¶æ®µ 1ï¼šSAE æ¨ç†** â€” åŠ è½½é¢„è®­ç»ƒçš„ Gemma Scope SAE æƒé‡ï¼Œè¿è¡Œæ¨ç†
- [ ] **é˜¶æ®µ 2ï¼šæ¿€æ´»å€¼æå–** â€” Hook è¿› Gemma æ¨¡å‹å„å±‚ï¼Œæå–æ¿€æ´»å€¼
- [ ] **é˜¶æ®µ 3ï¼šSAE è®­ç»ƒ** â€” å®ç° JumpReLU æŸå¤±å‡½æ•°çš„è®­ç»ƒå¾ªç¯ï¼ˆé‡å»º + ç¨€ç–æ€§ï¼‰
- [ ] **é˜¶æ®µ 4ï¼šè¯„ä¼°** â€” è®¡ç®— L0ã€FVU å’Œ Delta Loss æŒ‡æ ‡
- [ ] **é˜¶æ®µ 5ï¼šå¯è§£é‡Šæ€§å®éªŒ** â€” å¯è§†åŒ– top-activating ç‰¹å¾ï¼Œå®éªŒ Steering

## ğŸ”§ å‰ç½®æ¡ä»¶

- Python 3.10+
- CUDA 12.xï¼ˆA800 GPU è®­ç»ƒï¼‰
- ~16GB+ GPU æ˜¾å­˜ï¼ˆç”¨äº Gemma æ¨¡å‹ + SAEï¼‰
- HuggingFace Tokenï¼ˆä¸‹è½½ Gemma æ¨¡å‹éœ€è¦ï¼‰

## ğŸš€ å®Œæ•´ä½¿ç”¨æµç¨‹

### Step 0ï¼šç¯å¢ƒæ­å»º

```bash
git clone https://github.com/beauefui/GS-practice.git
cd GS-practice
conda create -n gs python=3.10 -y
conda activate gs
pip install torch --index-url https://download.pytorch.org/whl/cu121
pip install -r requirements.txt
```

### Step 1ï¼šä¸‹è½½æƒé‡

```bash
# å°† <YOUR_HF_TOKEN> æ›¿æ¢ä¸ºä½ çš„ token
# ä¸‹è½½ Gemma æ¨¡å‹ + Google é¢„è®­ç»ƒ SAE æƒé‡
python scripts/download_weights.py --token <YOUR_HF_TOKEN>
```

**å¾—åˆ°ï¼š** `model/gemma-3-1b-pt/` (Gemma åŸºåº§æ¨¡å‹) å’Œ `sae/gemma-scope-2-1b-pt/` (Google é¢„è®­ç»ƒ SAE æƒé‡)

### Step 2ï¼šSmoke Testï¼ˆéªŒè¯ç¯å¢ƒï¼‰

```bash
python scripts/train_sae.py --smoke-test
python scripts/eval_sae.py --smoke-test
```

**å¾—åˆ°ï¼š** ä½¿ç”¨éšæœºæ•°æ®è·‘å‡ æ­¥è®­ç»ƒå’Œè¯„ä¼°ï¼Œç¡®è®¤ç¯å¢ƒæ— é—®é¢˜

---

### ğŸ…°ï¸ ä¸»è·¯çº¿ï¼šä½¿ç”¨ Google é¢„è®­ç»ƒ SAE è¯„ä¼°ï¼ˆæ¨èï¼‰

> è¿™æ˜¯ä¸ [Colab æ•™ç¨‹](https://colab.research.google.com/drive/1NhWjg7n0nhfW--CjtsOdw5A5J_-Bzn4r) å¯¹é½çš„ç”¨æ³•ã€‚
> ç›´æ¥åŠ è½½ Google èŠ±å¤§é‡ç®—åŠ›è®­ç»ƒå¥½çš„ SAE æƒé‡ï¼Œå¯¹ Gemma æ¨¡å‹è¿›è¡Œåˆ†æã€‚

```bash
# ç›´æ¥è¯„ä¼° Google é¢„è®­ç»ƒ SAE
CUDA_VISIBLE_DEVICES=0 python scripts/eval_sae.py --pretrained
```

**è¿‡ç¨‹ï¼š**
1. åŠ è½½ Gemma æ¨¡å‹ + Google é¢„è®­ç»ƒ SAE æƒé‡ (`sae/gemma-scope-2-1b-pt/`)
2. æå–æ¿€æ´»å€¼ â†’ é€šè¿‡ SAE ç¼–ç /è§£ç  â†’ è®¡ç®—è¯„ä¼°æŒ‡æ ‡

**å¾—åˆ°ï¼š**
- ç»ˆç«¯æ‰“å°è¯„ä¼°æŠ¥å‘Šï¼ˆL0 ç¨€ç–åº¦ã€FVU é‡å»ºè´¨é‡ã€Top-10 æ´»è·ƒç‰¹å¾ï¼‰
- è‡ªåŠ¨ç”Ÿæˆ `reports/report_<æ—¶é—´æˆ³>.md` å’Œ `.json` æŠ¥å‘Šæ–‡ä»¶
- é¢„æœŸæ•ˆæœï¼š**L0 â‰ˆ 70, FVU â‰ˆ 2-3%**ï¼ˆä¸ Colab æ•™ç¨‹ä¸€è‡´ï¼‰

---

### ğŸ…±ï¸ å¯é€‰è·¯çº¿ï¼šä»é›¶è®­ç»ƒ SAEï¼ˆå­¦ä¹ ç”¨ï¼‰

> è¿™æ¡è·¯çº¿æ˜¯ä¸ºäº†**ç†è§£ SAE è®­ç»ƒè¿‡ç¨‹**ï¼Œæ•ˆæœè¿œä¸å¦‚ Google é¢„è®­ç»ƒç‰ˆæœ¬ï¼Œ
> ä½†å¯¹å­¦ä¹  SAE çš„å·¥ä½œåŸç†éå¸¸æœ‰å¸®åŠ©ã€‚

**è®­ç»ƒï¼š**

```bash
CUDA_VISIBLE_DEVICES=0 python scripts/train_sae.py --config configs/default.yaml
```

**è¯„ä¼°è‡ªè®­ç»ƒçš„ checkpointï¼š**

```bash
CUDA_VISIBLE_DEVICES=0 python scripts/eval_sae.py --checkpoint sae/checkpoints/checkpoint_final.pt
```

**è°ƒå‚ï¼ˆç¼–è¾‘ `configs/default.yaml`ï¼‰ï¼š**

```yaml
model:
  hook_layer: 22        # è¦ hook çš„å±‚ (0-25)
sae:
  d_sae: 16384          # SAE å®½åº¦
training:
  num_steps: 20000      # è®­ç»ƒæ­¥æ•°
  sparsity_coeff: 0.01  # ç¨€ç–æ€§å¼ºåº¦ (è¶Šå¤§è¶Šç¨€ç–)
  lr: 1e-4              # å­¦ä¹ ç‡
```

## ğŸ”„ åˆ‡æ¢ä¸åŒçš„ Gemma æ¨¡å‹å’Œ SAE

### å¯ç”¨çš„æ¨¡å‹å’Œå¯¹åº”çš„ Gemma Scope

æ¯ä¸ª Gemma åŸºåº§æ¨¡å‹éƒ½æœ‰å¯¹åº”çš„ Gemma Scope SAE æƒé‡ï¼ˆ`-pt` = é¢„è®­ç»ƒç‰ˆï¼Œ`-it` = æŒ‡ä»¤å¾®è°ƒç‰ˆï¼‰ï¼š

| Gemma åŸºåº§æ¨¡å‹ | å¯¹åº” Gemma Scope | å±‚æ•° | d_model | æ˜¾å­˜éœ€æ±‚ |
|---------------|-----------------|------|---------|---------|
| `google/gemma-3-270m-pt` | `google/gemma-scope-2-270m-pt` | 18 | 1536 | ~2 GB |
| `google/gemma-3-1b-pt` â† **å½“å‰** | `google/gemma-scope-2-1b-pt` | 26 | 1152 | ~4 GB |
| `google/gemma-3-4b-pt` | `google/gemma-scope-2-4b-pt` | 34 | 2560 | ~10 GB |
| `google/gemma-3-12b-pt` | `google/gemma-scope-2-12b-pt` | 48 | 3840 | ~28 GB |
| `google/gemma-3-27b-pt` | `google/gemma-scope-2-27b-pt` | 62 | 4608 | ~60 GB |

> æŠŠ `-pt` æ¢æˆ `-it` å³å¯ä½¿ç”¨æŒ‡ä»¤å¾®è°ƒç‰ˆæœ¬ï¼ˆå¦‚ `gemma-3-4b-it` + `gemma-scope-2-4b-it`ï¼‰

### åˆ‡æ¢æ­¥éª¤

**ä»¥åˆ‡æ¢åˆ° 4B æ¨¡å‹ä¸ºä¾‹ï¼š**

#### 1. ä¿®æ”¹ä¸‹è½½è„šæœ¬ `scripts/download_weights.py`

```python
# æ”¹ repo_id å’Œ local_dir
snapshot_download(
    repo_id="google/gemma-3-4b-pt",          # â† æ”¹è¿™é‡Œ
    local_dir="model/gemma-3-4b-pt",          # â† æ”¹è¿™é‡Œ
    token=args.token,
)

snapshot_download(
    repo_id="google/gemma-scope-2-4b-pt",     # â† æ”¹è¿™é‡Œ
    local_dir="sae/gemma-scope-2-4b-pt",      # â† æ”¹è¿™é‡Œ
    allow_patterns=["resid_post/layer_20_width_65k_l0_medium/*"],  # â† æ”¹å±‚å·
    token=args.token,
)
```

#### 2. ä¿®æ”¹é…ç½®æ–‡ä»¶ `configs/default.yaml`

```yaml
model:
  name: "model/gemma-3-4b-pt"     # â† æ”¹æ¨¡å‹è·¯å¾„
  hook_layer: 20                   # â† æ”¹å±‚å· (é€šå¸¸é€‰ä¸­é—´ååçš„å±‚)

pretrained_sae:
  repo_id: "google/gemma-scope-2-4b-pt"    # â† æ”¹ scope ä»“åº“
  local_dir: "sae/gemma-scope-2-4b-pt"     # â† æ”¹æœ¬åœ°è·¯å¾„
  layer: 20                                 # â† å’Œ hook_layer ä¸€è‡´
  width: "65k"
  l0: "medium"
```

#### 3. é‡æ–°ä¸‹è½½å¹¶è¯„ä¼°

```bash
python scripts/download_weights.py --token <YOUR_HF_TOKEN>
CUDA_VISIBLE_DEVICES=0 python scripts/eval_sae.py --pretrained
```

### SAE å˜ä½“é€‰æ‹©

æ¯ä¸ªå±‚ä¸‹æœ‰ä¸åŒå®½åº¦å’Œç¨€ç–åº¦çš„ SAE å¯é€‰ï¼Œåœ¨ `allow_patterns` å’Œ `configs/default.yaml` ä¸­ä¿®æ”¹ï¼š

| å‚æ•° | å¯é€‰å€¼ | è¯´æ˜ |
|------|-------|------|
| `width` | `16k`, `65k`, `262k`, `1m` | ç‰¹å¾æ•°é‡ï¼Œè¶Šå¤§è¶Šç»†ç²’åº¦ |
| `l0` | `small`, `medium`, `big` | ç›®æ ‡ç¨€ç–åº¦ï¼Œsmall=æ›´ç¨€ç– |

> ä¾‹å¦‚ `layer_15_width_262k_l0_small` è¡¨ç¤ºç¬¬ 15 å±‚ã€262k ç‰¹å¾ã€é«˜ç¨€ç–åº¦

## ğŸ¦™ æ‹“å±•ï¼šä» Gemma Scope åˆ° Llama Scope

[Llama Scope](https://github.com/OpenMOSS/Language-Model-SAEs) æ˜¯ OpenMOSS å›¢é˜Ÿä¸º **Llama-3.1-8B** è®­ç»ƒçš„ SAE å¥—ä»¶ï¼Œæä¾›äº†æ‰€æœ‰å±‚å’Œå­å±‚çš„ 256 ä¸ª TopK SAEã€‚

### Gemma Scope vs Llama Scope æ ¸å¿ƒåŒºåˆ«

| | Gemma Scope | Llama Scope |
|---|---|---|
| **åŸºåº§æ¨¡å‹** | Gemma 3 (270M ~ 27B) | Llama 3.1 8B |
| **SAE æ¶æ„** | JumpReLU (å¯å­¦ä¹ é˜ˆå€¼) | **TopK** (å›ºå®šé€‰ top-k ä¸ªç‰¹å¾) |
| **æƒé‡æ¥æº** | `google/gemma-scope-2-*` | `fnlp/Llama-Scope` |
| **ç‰¹å¾æ•°é‡** | 16k / 65k / 262k / 1m | **32k (8x)** / 128k (32x) |
| **å‘½åè§„åˆ™** | `layer_22_width_65k_l0_medium` | `L22R-8x` (å±‚å·+ä½ç½®+å€ç‡) |

#### Llama Scope å‘½åè§„åˆ™

`L[å±‚å·][ä½ç½®]-[å€ç‡]x`ï¼š

| ä½ç½®ä»£ç  | å«ä¹‰ | å¯¹åº” Gemma Scope |
|---------|------|-----------------|
| `R` | post-MLP **R**esidual stream | `resid_post` |
| `A` | **A**ttention output | `attn_output` |
| `M` | **M**LP output | `mlp_output` |

> ä¾‹å¦‚ `L15R-8x` = ç¬¬ 15 å±‚çš„ Residual streamï¼Œ8x æ‰©å±• (32k ç‰¹å¾)
> âš ï¸ `32x` çš„ SAE (128k ç‰¹å¾) æ­»ç‰¹å¾è¾ƒå¤šï¼Œ**æ¨èç”¨ `8x` (32k ç‰¹å¾)**

---

### æ–¹æ³• Aï¼šä½¿ç”¨ `lm-saes` æ¡†æ¶ï¼ˆæ¨èï¼‰

> è¿™æ˜¯ Llama Scope å®˜æ–¹æ¨èçš„æ–¹å¼ï¼Œé€‚åˆæ·±å…¥ç ”ç©¶ã€‚

#### Step 1ï¼šåˆ›å»ºæ–°ç¯å¢ƒå¹¶å®‰è£…

```bash
# å»ºè®®æ–°å»ºä¸€ä¸ª conda ç¯å¢ƒï¼Œé¿å…ä¸ Gemma Scope ä¾èµ–å†²çª
conda create -n llama-scope python=3.10 -y
conda activate llama-scope

# å®‰è£… PyTorch
pip install torch --index-url https://download.pytorch.org/whl/cu121

# å®‰è£… lm-saes æ¡†æ¶
pip install lm-saes==2.0.0b16
```

#### Step 2ï¼šä¸‹è½½ Llama æ¨¡å‹å’Œ SAE æƒé‡

```python
# æ–°å»ºä¸€ä¸ªè„šæœ¬: scripts/download_llama_scope.py
from huggingface_hub import snapshot_download

# 1. ä¸‹è½½ Llama 3.1 8B åŸºåº§æ¨¡å‹ (çº¦ 16GB)
snapshot_download(
    repo_id="meta-llama/Llama-3.1-8B",
    local_dir="model/Llama-3.1-8B",
    token="<YOUR_HF_TOKEN>",
)

# 2. ä¸‹è½½ Llama Scope SAE æƒé‡ (åªä¸‹éœ€è¦çš„ä¸€ä¸ª)
snapshot_download(
    repo_id="fnlp/Llama-Scope",
    allow_patterns=["L15R-8x/*"],   # ç¬¬15å±‚ Residual 8x, æŒ‰éœ€ä¿®æ”¹
    local_dir="sae/llama-scope",
    token="<YOUR_HF_TOKEN>",
)
```

```bash
python scripts/download_llama_scope.py
```

#### Step 3ï¼šä½¿ç”¨ lm-saes åŠ è½½å’Œè¯„ä¼°

```python
# æ–°å»ºä¸€ä¸ªè„šæœ¬: scripts/eval_llama_scope.py
from lm_saes import SparseAutoEncoder
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# åŠ è½½ Llama æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "model/Llama-3.1-8B",
    dtype=torch.bfloat16,
    device_map="auto",
)
tokenizer = AutoTokenizer.from_pretrained("model/Llama-3.1-8B")

# åŠ è½½ Llama Scope SAE
sae = SparseAutoEncoder.from_pretrained("sae/llama-scope/L15R-8x")
sae = sae.to("cuda")

print(f"SAE åŠ è½½å®Œæˆ: {sae}")
print(f"  d_model: {sae.d_model}")
print(f"  d_sae:   {sae.d_sae}")

# æå–æ¿€æ´»å€¼å¹¶é€šè¿‡ SAE ç¼–ç 
text = "The quick brown fox jumps over the lazy dog."
inputs = tokenizer(text, return_tensors="pt").to("cuda")

with torch.no_grad():
    outputs = model(**inputs, output_hidden_states=True)
    # ç¬¬ 15 å±‚çš„ hidden states (0-indexed, +1 å› ä¸ºåŒ…å« embedding å±‚)
    activations = outputs.hidden_states[16].float()  # (1, seq_len, d_model)

    # é€šè¿‡ SAE ç¼–ç /è§£ç 
    acts = activations.reshape(-1, activations.shape[-1])  # (seq_len, d_model)
    encoded = sae.encode(acts)
    decoded = sae.decode(encoded)

    # è®¡ç®—æŒ‡æ ‡
    l0 = (encoded > 0).float().sum(dim=-1).mean().item()
    fvu = ((acts - decoded).pow(2).sum() / acts.pow(2).sum()).item()

print(f"\nè¯„ä¼°ç»“æœ:")
print(f"  L0 (ç¨€ç–åº¦): {l0:.1f}")
print(f"  FVU (é‡å»ºè¯¯å·®): {fvu:.4f}")
```

```bash
CUDA_VISIBLE_DEVICES=0 python scripts/eval_llama_scope.py
```

---

### æ–¹æ³• Bï¼šæ”¹é€ æˆ‘ä»¬çš„ä»£ç ï¼ˆå­¦ä¹ ç”¨ï¼‰

> å¦‚æœä½ æƒ³ç”¨æœ¬é¡¹ç›®çš„ä»£ç æ¡†æ¶æ¥åŠ è½½ Llama Scopeï¼Œéœ€è¦åšä»¥ä¸‹ä¿®æ”¹ã€‚

#### Step 1. `src/model.py` â€” æ–°å¢ TopKSAE ç±»

åœ¨ `JumpReLUSAE` ç±»ä¸‹é¢æ·»åŠ ä¸€ä¸ªæ–°çš„ SAE ç±»ï¼š

```python
class TopKSAE(nn.Module):
    """TopK SAE (Llama Scope ä½¿ç”¨çš„æ¶æ„)

    ä¸ JumpReLU çš„åŒºåˆ«:
      - JumpReLU: æ¯ä¸ªç‰¹å¾æœ‰å¯å­¦ä¹ é˜ˆå€¼, ä½äºé˜ˆå€¼çš„ç½®é›¶
      - TopK: å›ºå®šé€‰å‰ k ä¸ªæœ€å¤§çš„ç‰¹å¾, å…¶ä½™ç½®é›¶ (k æ˜¯è¶…å‚æ•°, ä¸å¯å­¦)
    """
    def __init__(self, d_model: int, d_sae: int, k: int = 64):
        super().__init__()
        self.d_model = d_model
        self.d_sae = d_sae
        self.k = k

        self.W_enc = nn.Parameter(torch.empty(d_model, d_sae))
        self.b_enc = nn.Parameter(torch.zeros(d_sae))
        self.W_dec = nn.Parameter(torch.empty(d_sae, d_model))
        self.b_dec = nn.Parameter(torch.zeros(d_model))

    def encode(self, x):
        pre_acts = x @ self.W_enc + self.b_enc
        # TopK: åªä¿ç•™å‰ k ä¸ªæœ€å¤§å€¼
        topk_vals, topk_idx = pre_acts.topk(self.k, dim=-1)
        acts = torch.zeros_like(pre_acts)
        acts.scatter_(-1, topk_idx, topk_vals)
        return acts

    def decode(self, acts):
        return acts @ self.W_dec + self.b_dec

    def forward(self, x):
        acts = self.encode(x)
        recon = self.decode(acts)
        return recon, acts
```

#### Step 2. `scripts/download_weights.py` â€” æ·»åŠ  Llama ä¸‹è½½

åœ¨ `main()` ä¸­æ·»åŠ  Llama çš„ä¸‹è½½é€»è¾‘ï¼ˆæˆ–æ–°å»ºè„šæœ¬ï¼‰ï¼š

```python
# ä¸‹è½½ Llama 3.1 8B
snapshot_download(
    repo_id="meta-llama/Llama-3.1-8B",
    local_dir="model/Llama-3.1-8B",
    token=args.token,
)

# ä¸‹è½½ Llama Scope SAE
snapshot_download(
    repo_id="fnlp/Llama-Scope",
    allow_patterns=["L15R-8x/*"],
    local_dir="sae/llama-scope",
    token=args.token,
)
```

#### Step 3. `src/utils.py` â€” æ·»åŠ  Llama Scope æƒé‡åŠ è½½å‡½æ•°

```python
def load_llama_scope_weights(
    local_dir: str = "sae/llama-scope",
    sae_name: str = "L15R-8x",
) -> dict:
    """åŠ è½½ Llama Scope SAE æƒé‡"""
    from safetensors.torch import load_file
    path = Path(local_dir) / sae_name / "model.safetensors"
    if not path.exists():
        raise FileNotFoundError(f"æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {path}")
    params = load_file(str(path))
    print(f"[Llama Scope] åŠ è½½å®Œæˆ: {sae_name}")
    for k, v in params.items():
        print(f"  {k}: {v.shape}")
    return params
```

#### Step 4. `src/hooks.py` â€” ä¸éœ€è¦æ”¹

Llama å’Œ Gemma ç»“æ„ç›¸åŒï¼Œéƒ½æ˜¯ `model.model.layers[i]`ï¼Œhooks ä»£ç **å®Œå…¨é€šç”¨**ã€‚

#### Step 5. `configs/default.yaml` â€” æ”¹é…ç½®

```yaml
model:
  name: "model/Llama-3.1-8B"
  hook_layer: 15
  dtype: "bfloat16"

pretrained_sae:
  local_dir: "sae/llama-scope"
  sae_name: "L15R-8x"
```

---

### å¯¹ç…§å‚è€ƒ

| æ¥æº | é“¾æ¥ |
|------|------|
| **Llama Scope è®ºæ–‡** | [arxiv.org/abs/2410.20526](https://arxiv.org/abs/2410.20526) |
| **è®­ç»ƒæ¡†æ¶** | [github.com/OpenMOSS/Language-Model-SAEs](https://github.com/OpenMOSS/Language-Model-SAEs) |
| **é¢„è®­ç»ƒæƒé‡** | [huggingface.co/fnlp/Llama-Scope](https://huggingface.co/fnlp/Llama-Scope) |


## ğŸ“¦ ä¸»è¦ä¾èµ–

| åŒ…å | ç”¨é€” |
|------|------|
| `torch` | æ·±åº¦å­¦ä¹ æ¡†æ¶ |
| `transformers` | ä» HuggingFace åŠ è½½ Gemma æ¨¡å‹ |
| `huggingface_hub` | ä¸‹è½½ SAE æƒé‡ |
| `safetensors` | é«˜æ•ˆçš„æƒé‡åºåˆ—åŒ–æ ¼å¼ |
| `einops` | å¼ é‡è¿ç®— |
| `pyyaml` | é…ç½®æ–‡ä»¶è§£æ |
| `datasets` | åŠ è½½è®­ç»ƒæ•°æ®é›† |
| `wandb` | å®éªŒè¿½è¸ªï¼ˆå¯é€‰ï¼‰ |

## ğŸ“– å‚è€ƒèµ„æ–™

- **Gemma Scope åšå®¢**: [deepmind.google/gemma-scope](https://deepmind.google/discover/blog/gemma-scope-helping-the-safety-community-shed-light-on-the-inner-workings-of-language-models/)
- **Gemma Scope 2 Tutorial (Colab)**: [colab.research.google.com](https://colab.research.google.com/drive/1NhWjg7n0nhfW--CjtsOdw5A5J_-Bzn4r)
- **JumpReLU è®ºæ–‡**: [Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders](https://arxiv.org/abs/2407.14435)
- **Gemma Scope æƒé‡ (HuggingFace)**: [google/gemma-scope-2b-pt-res](https://huggingface.co/google/gemma-scope-2b-pt-res)
- **SAELens (è®­ç»ƒåº“)**: [github.com/jbloomAus/SAELens](https://github.com/jbloomAus/SAELens)

## âš–ï¸ è®¸å¯

æœ¬é¡¹ç›®ä»…ç”¨äºä¸ªäººå­¦ä¹ å’Œç ”ç©¶ç›®çš„ã€‚
